# Word2Vec and Average Word2Vec Implementation

## Description
This project demonstrates **Word2Vec and Average Word2Vec** techniques used in **Natural Language Processing (NLP)**. Word2Vec models convert words into dense vector representations that capture semantic meanings, while Average Word2Vec aggregates word embeddings to represent entire sentences or documents.

## Overview
This Jupyter Notebook covers:
- **Word2Vec Implementation**: Training a Word2Vec model using Gensim.
- **Average Word2Vec**: Computing document-level embeddings by averaging word vectors.
- **Data Preprocessing**: Tokenization, stopword removal, and cleaning.

## Requirements
To run this notebook, install the necessary dependencies:
```bash
pip install gensim nltk numpy pandas
```

## Usage
1. Open the notebook in Jupyter:
   ```bash
   jupyter notebook NLP_Word2vec_And_AvgWord2vec.ipynb
   ```
2. Run the cells step by step to understand and train the models.
3. Modify input text data to experiment with different word embeddings.

## Implementation Details
- **Gensim**: Used for Word2Vec implementation.
- **NLTK**: Used for text preprocessing.
- **NumPy & Pandas**: Used for handling data and numerical computations.

